{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b3c744c-3989-48a3-a137-b7ea0587a2e8",
   "metadata": {},
   "source": [
    "# Email Spam Detection with Machine Learning\n",
    "\n",
    "Spam emails, also known as junk mail, pose a significant problem for email users worldwide. These unsolicited messages often contain fraudulent content, phishing attempts, or unwanted advertisements, leading to a cluttered inbox and potential security risks. In this project, we'll explore how to build a machine learning model for email spam detection using Python.\n",
    "\n",
    "## About the Dataset\n",
    "\n",
    "The dataset used in this project is the SMS Spam Collection, which consists of SMS messages tagged as either \"ham\" (legitimate) or \"spam\". It contains 5,574 messages in English, collected from various sources on the internet. Each message is labeled with its corresponding class (ham or spam), allowing us to train a classifier to distinguish between legitimate and spam emails.\n",
    "\n",
    "## Goal of the Project\n",
    "\n",
    "Our goal is to develop a machine learning model capable of accurately classifying emails as either spam or ham. We'll start by preprocessing the text data, including tokenization, removal of punctuation, and elimination of stopwords. Next, we'll create a Bag-of-Words representation of the text data and split the dataset into training and testing sets. Finally, we'll train a Multinomial Naive Bayes classifier on the training data and evaluate its performance on the testing set.\n",
    "\n",
    "By the end of this project, we'll have a trained model that can effectively identify spam emails, helping users filter out unwanted content and enhance their email security.\n",
    "\n",
    "Let's dive into the implementation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb976d89-a567-46f4-9d28-96d79b1f16b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df = df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "\n",
    "# Display the first few rows of the dataset again to confirm the changes\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5ab35011-6c47-412b-ba68-ab0d60cc98d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                           v2_tokens  \\\n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
      "\n",
      "                                     v2_concatenated  \n",
      "0  Go until jurong point , crazy .. Available onl...  \n",
      "1                    Ok lar ... Joking wif u oni ...  \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
      "3  U dun say so early hor ... U c already then sa...  \n",
      "4  Nah I do n't think he goes to usf , he lives a...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text data\n",
    "df['v2_tokens'] = df['v2'].apply(word_tokenize)\n",
    "\n",
    "# Concatenate the tokens back into a single string for each document\n",
    "df['v2_concatenated'] = df['v2_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "05833e65-7cb3-46a9-b2be-39517fa4ef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80618.37s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "                                                  v2  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                           v2_tokens  \n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...  \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]  \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...  \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...  \n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the text data\n",
    "df['v2_tokens'] = df['v2'].apply(word_tokenize)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df[['v2', 'v2_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bfeb59a-bed6-417d-87b2-4cf9e5e4ecfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                           v2_tokens  \\\n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
      "\n",
      "                                     v2_concatenated  \\\n",
      "0  Go until jurong point , crazy .. Available onl...   \n",
      "1                    Ok lar ... Joking wif u oni ...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor ... U c already then sa...   \n",
      "4  Nah I do n't think he goes to usf , he lives a...   \n",
      "\n",
      "                                          v2_cleaned  \n",
      "0  [Go, until, jurong, point, , crazy, , Availabl...  \n",
      "1                 [Ok, lar, , Joking, wif, u, oni, ]  \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
      "3  [U, dun, say, so, early, hor, , U, c, already,...  \n",
      "4  [Nah, I, do, nt, think, he, goes, to, usf, , h...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    # Define the pattern to match any punctuation marks\n",
    "    punctuation_pattern = re.compile(r'[^\\w\\s]')\n",
    "    \n",
    "    # Remove punctuation from each token\n",
    "    tokens_without_punctuation = [re.sub(punctuation_pattern, '', token) for token in tokens]\n",
    "    \n",
    "    return tokens_without_punctuation\n",
    "\n",
    "# Apply the function to each tokenized message\n",
    "df['v2_cleaned'] = df['v2_tokens'].apply(remove_punctuation)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60ff65da-c0d3-426a-a989-4a742b40174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                           v2_tokens  \\\n",
      "0  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
      "\n",
      "                                     v2_concatenated  \\\n",
      "0  Go until jurong point , crazy .. Available onl...   \n",
      "1                    Ok lar ... Joking wif u oni ...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor ... U c already then sa...   \n",
      "4  Nah I do n't think he goes to usf , he lives a...   \n",
      "\n",
      "                                          v2_cleaned  \n",
      "0  [Go, jurong, point, , crazy, , Available, bugi...  \n",
      "1                 [Ok, lar, , Joking, wif, u, oni, ]  \n",
      "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
      "3  [U, dun, say, early, hor, , U, c, already, say, ]  \n",
      "4  [Nah, nt, think, goes, usf, , lives, around, t...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/95b32ad5-31de-4480-bb89-\n",
      "[nltk_data]     b070e00f8fb3/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Apply remove_stopwords function to each row in the v2_cleaned column\n",
    "df['v2_cleaned'] = df['v2_cleaned'].apply(remove_stopwords)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e9d96adb-ab29-4947-806f-2e227d89c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW matrix shape: (5572, 8660)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "bow_matrix = count_vectorizer.fit_transform(df['v2_concatenated'])\n",
    "\n",
    "# Convert the BoW matrix to an array for easier handling\n",
    "bow_array = bow_matrix.toarray()\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"BoW matrix shape:\", bow_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "891cdecc-ec04-4e31-90b3-b75314c3cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4457,)\n",
      "X_test shape: (1115,)\n",
      "y_train shape: (4457,)\n",
      "y_test shape: (1115,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df['v2_cleaned']\n",
    "y = df['v1']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "88597934-3858-46ba-8c34-5993e85cfbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9838565022421525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       965\n",
      "        spam       0.99      0.89      0.94       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.95      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "naive_bayes_classifier.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = naive_bayes_classifier.predict(X_test_bow)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eeaca9-3f87-4ed3-9316-ddcba558d851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e7085ac-c293-4a43-a85b-33c3f25825fa",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project, we developed a machine learning model for email spam detection using the SMS Spam Collection dataset. We started by preprocessing the text data, which involved tokenization, removal of punctuation, and elimination of stopwords. We then created a Bag-of-Words representation of the text data and split the dataset into training and testing sets.\n",
    "\n",
    "The Bag-of-Words matrix had a shape of (5572, 8660), indicating that there were 5,572 messages in the dataset, with 8,660 unique words across all messages. The training set consisted of 4,457 samples, while the testing set comprised 1,115 samples.\n",
    "\n",
    "We trained a Multinomial Naive Bayes classifier on the training data and evaluated its performance on the testing set. The classifier achieved an impressive accuracy of 98.39% on the testing set. Additionally, the classification report showed high precision, recall, and F1-score for both the \"ham\" (legitimate) and \"spam\" classes.\n",
    "\n",
    "Overall, the results demonstrate the effectiveness of the machine learning model in accurately classifying emails as either spam or ham. Such a model can be valuable for email users in filtering out unwanted spam content and improving email security.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92fcbe1-d1fd-429d-9aa0-332bd6f4e437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
